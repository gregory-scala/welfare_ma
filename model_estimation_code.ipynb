{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7be9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np; #importing packages we will use\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from scipy.io import loadmat\n",
    "import os \n",
    "import scipy.linalg as la\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import pickle\n",
    "import time\n",
    "import concurrent.futures\n",
    "import cProfile\n",
    "import random as rd\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from joblib import Parallel,delayed\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import h5py\n",
    "import pstats\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "\n",
    "###importing data dictionaries I will be using \n",
    "os.chdir('G:\\\\Greg\\\\full_model')\n",
    "with open('main_dictionary_2019.pkl', 'rb') as fp:\n",
    "    main_dictionary = pickle.load(fp)\n",
    "with open('useful_variables.pkl', 'rb') as fp:\n",
    "    usable_county_years = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c5fe1",
   "metadata": {},
   "source": [
    "### Visualization of the matrix I will be using to interact demographics with individuals\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "t_2 & t_3 & nh\\_black & hispanic \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\theta_{2,int} & \\theta_{2,p} & \\theta_{2,cs} & \\theta_{2,d} & \\theta_{2,v} & \\theta_{2,h}\\\\\n",
    "\\theta_{3,int} & \\theta_{3,p} & \\theta_{3,cs} & \\theta_{3,d} & \\theta_{3,v} & \\theta_{3,h}\\\\\n",
    "\\theta_{b,int} & \\theta_{b,p} & \\theta_{b,cs} & \\theta_{b,d} & \\theta_{b,v} & \\theta_{b,h}\\\\\n",
    "\\theta_{hi,int} & \\theta_{hi,p} & \\theta_{hi,cs} & \\theta_{hi,d} & \\theta_{hi,v} & \\theta_{hi,h}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ p \\\\ cs \\\\ d \\\\ v \\\\h\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* Plan/individual interactions needs to be a 4x6 matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f927c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###using initial guess for parameters based on a one-county model I made \n",
    "\n",
    "with open('initial_estimates_county_2019.pkl', 'rb') as fp:\n",
    "    old_guess = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9c7e7",
   "metadata": {},
   "source": [
    "### Functions to calculate Model Predicted Market Shares and Plan choice probabilities for every individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b704e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in individual-level data, plan-level data, an additional guess for the mean uitility values, and the parameter values\n",
    "# calculates the model predicted market share of each plan in each county\n",
    "def mkt_share_testing_b(individual_data_1, individual_data_2, plan_data, lin_util_val, guess_util):\n",
    "    \n",
    "    ##preparing parameter guesses\n",
    "    guess_util_1 = guess_util[0:24].reshape(4,-1) #first 24 parameters into a 4x6 matrix\n",
    "    guess_util_2 = guess_util[24:]\n",
    "    \n",
    "    #gets the plan_attributes for each plan in the county (and including an intercept)\n",
    "    ones_array = np.ones((plan_data.shape[0],1))\n",
    "    ptemp = np.hstack((ones_array, plan_data))\n",
    "    \n",
    "    #takes calculates nonlinear utility for each individual for all combinations of plans in each county\n",
    "    nonlin_guess1 = individual_data_1@(guess_util_1)@(ptemp.T)\n",
    "    nonlin_guess2a = individual_data_2@(guess_util_2.reshape(-1,1))\n",
    "    nonlin_guess2 = np.broadcast_to(nonlin_guess2a, (nonlin_guess2a.shape[0], plan_data.shape[0]))\n",
    "    nonlin_guess = nonlin_guess1 + nonlin_guess2\n",
    "    \n",
    "   #Creating a linear utility matrix for all individuals, for each plan\n",
    "    lin_util_vec = lin_util_val.reshape((-1,1)).T\n",
    "    lin_vec_full = np.tile(lin_util_vec, (nonlin_guess.shape[0],1)) #broadcasts lin_util_vec to all rows of individuals\n",
    "    \n",
    "    #adding linear and nonlinear utility for each plan\n",
    "    full_log_util = nonlin_guess + lin_vec_full \n",
    "    \n",
    "    #calculating individual choice probability of each plan\n",
    "    temp1 = np.zeros((len(full_log_util[:,0]),1))\n",
    "    full_log_util2 = np.hstack((temp1, full_log_util))\n",
    "    lsumexp = sp.special.logsumexp(full_log_util2, axis=1, keepdims=True)\n",
    "    lsumexp2 = np.tile(lsumexp, (1,len(full_log_util2[1])))\n",
    "    predicted_choice_probability = np.exp(full_log_util2 - lsumexp2)[:,1:] #getting the pcp of all non-outside_option plans\n",
    "    \n",
    "    #calculating model predicted share of all MA plans \n",
    "    model_predicted_share = (1/len(predicted_choice_probability))*predicted_choice_probability.sum(axis=0)\n",
    "\n",
    "    return model_predicted_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7947dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in individual-level data, plan-level data, an additional guess for the mean uitility values, and the parameter values\n",
    "# calculates the model predicted probability each individual chooses each plan in a county\n",
    "\n",
    "def pcp_matrix_county_b(temp_ind_1, temp_ind_2, ptemp1, lin_util_val, guess_util):\n",
    "    \n",
    "    ##preparing parameter guesses\n",
    "    guess_util_1 = guess_util[0:24].reshape(4,-1)\n",
    "    guess_util_2 = guess_util[24:]\n",
    "    \n",
    "    plan_vars = ['eff_price_adjusted', 'costsharing_pmpm_adjusted','dental_coverage_indicator', \n",
    "                 'eyewear_coverage_indicator', 'hearing_aides_coverage_indicator']\n",
    "\n",
    "    #gets the plan_attributes for each plan in the county (and including an intercept)\n",
    "    ones_array = np.ones((ptemp1.shape[0],1))\n",
    "    ptemp = np.hstack((ones_array, ptemp1))\n",
    "    \n",
    "   #takes calculates nonlinear utility for each individual for all combinations of plans in each county\n",
    "    nonlin_guess1 = temp_ind_1@(guess_util_1)@(ptemp.T)\n",
    "    nonlin_guess2a = temp_ind_2@(guess_util_2.reshape(-1,1))\n",
    "    nonlin_guess2 = np.broadcast_to(nonlin_guess2a, (nonlin_guess2a.shape[0], ptemp1.shape[0]))\n",
    "    nonlin_guess = nonlin_guess1 + nonlin_guess2\n",
    "    \n",
    "   #Creating a linear utility matrix for all individuals, for each plan\n",
    "    lin_util_vec = lin_util_val.reshape((-1,1)).T\n",
    "    lin_vec_full = np.kron(np.ones((len(nonlin_guess),1)),lin_util_vec) #broadcasts lin_util_vec to all rows of individuals\n",
    "    \n",
    "    #adding linear and nonlinear utility for each plan\n",
    "    full_log_util = nonlin_guess + lin_vec_full \n",
    "    \n",
    "    #calculating choice probabilities using logsumexp trick\n",
    "    temp1 = np.zeros((len(full_log_util[:,0]),1))\n",
    "    full_log_util2 = np.hstack((temp1, full_log_util))\n",
    "    lsumexp = sp.special.logsumexp(full_log_util2, axis=1)\n",
    "    lsumexp2 = np.kron(lsumexp.reshape(-1,1),np.ones((1, full_log_util2.shape[1])))\n",
    "    predicted_choice_probability_full = np.exp(full_log_util2 - lsumexp2)\n",
    "\n",
    "    return predicted_choice_probability_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8f0ee",
   "metadata": {},
   "source": [
    "## Contraction Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "638969f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraction_mapping_b(data_dictionary, usable_county_years, guess_util, tol):\n",
    "    \n",
    "    data_dictionary2 = copy.deepcopy(data_dictionary)\n",
    "    \n",
    "    # solves contraction mapping county by county\n",
    "    for county in usable_county_years:\n",
    "        ##get individual data and plan_data to use\n",
    "        plan_vars = ['eff_price_adjusted', 'costsharing_pmpm_adjusted','dental_coverage_indicator', \n",
    "                     'eyewear_coverage_indicator', 'hearing_aides_coverage_indicator']\n",
    "        \n",
    "        #gets the individual data\n",
    "        temp_ind_1 = data_dictionary2[county]['individual_data'][['income_tercile_2.0', 'income_tercile_3.0',\n",
    "                                                                 'nh_black', 'hispanic']].values\n",
    "        temp_ind_2 = data_dictionary2[county]['individual_data'][['employer_plan','h_age','h_age_sq','healthcode_1',\n",
    "                                                                 'healthcode_2', 'healthcode_3', 'healthcode_5']].values\n",
    "        #gets the plan_attributes for each plan in the county\n",
    "        ptemp1 = data_dictionary2[county]['plan_data'][plan_vars].values\n",
    "        \n",
    "        #gets the initial linear utility values from berry1994 and calculates the predicted county shares\n",
    "        test1 = data_dictionary2[county]['plan_data']['lin_util_init'].values\n",
    "        guess_mkt_share = mkt_share_testing_b(temp_ind_1, temp_ind_2, ptemp1, test1, guess_util)\n",
    "\n",
    "\n",
    "        #gets true market share values to use in contraction mapping\n",
    "        s = data_dictionary2[county]['plan_data']['plan_market_share'].values\n",
    "        #temp_data['model_share'] = guess_mkt_share\n",
    "\n",
    "        #solves for mean utility values (in test2) that match model predicted shares to observed market shares in actual data\n",
    "        #using contraction mapping\n",
    "        eps = 1000\n",
    "        while eps > tol:\n",
    "            bob = mkt_share_testing_b(temp_ind_1, temp_ind_2, ptemp1,test1, guess_util)\n",
    "\n",
    "            if np.isnan(bob).any():\n",
    "                return \"Bad Parameter Guess\"\n",
    "\n",
    "            test2 = test1 + np.log(s/bob)\n",
    "\n",
    "            eps = np.max(np.abs(test2-test1))\n",
    "            test1 = test2.copy()\n",
    "\n",
    "        data_dictionary2[county]['plan_data']['mean_util_guess'] = test2\n",
    "        \n",
    "    #returns updated dictionary with the new average utilities\n",
    "    return data_dictionary2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc509b6",
   "metadata": {},
   "source": [
    "## Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d177d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_function_b(dict,usable_county_years,guess_util):\n",
    "    \n",
    "    dict1 = dict.copy()\n",
    "    \n",
    "    #calculating sample size\n",
    "    N=0\n",
    "    likelihood_vec = np.array([])\n",
    "    \n",
    "    \n",
    "    plan_vars = ['eff_price_adjusted', 'costsharing_pmpm_adjusted','dental_coverage_indicator', 'eyewear_coverage_indicator', \n",
    "                 'hearing_aides_coverage_indicator']\n",
    "    \n",
    "    #sums the weighted log-likelihood county by county\n",
    "    for county in usable_county_years:\n",
    "        \n",
    "        ##gets individual data for use in the likelihood function\n",
    "        weights = dict1[county]['individual_data']['weight_scaled'].values\n",
    "        guess_lin_util = dict1[county]['plan_data']['mean_util_guess'].values\n",
    "        \n",
    "        temp_ind_1 = dict1[county]['individual_data'][['income_tercile_2.0', 'income_tercile_3.0',\n",
    "                                                                 'nh_black', 'hispanic']].values\n",
    "        temp_ind_2 = dict1[county]['individual_data'][['employer_plan','h_age','h_age_sq','healthcode_1',\n",
    "                                                                 'healthcode_2', 'healthcode_3', 'healthcode_5']].values\n",
    "\n",
    "\n",
    "        #gets the plan_attributes for each plan in the county\n",
    "        ptemp_1 = dict1[county]['plan_data'][plan_vars].values\n",
    "        \n",
    "        pcp_full = pcp_matrix_county_b(temp_ind_1, temp_ind_2, ptemp_1, guess_lin_util, guess_util)\n",
    "\n",
    "        m,n = pcp_full.shape\n",
    "            \n",
    "        #getting matrix of all plans offered in each county\n",
    "        plans_matrix  = dict1[county]['plan_data']['merge_variable'].values\n",
    "        OO = np.array(['OO'])\n",
    "        plans_matrixOO = np.concatenate((OO,plans_matrix))\n",
    "\n",
    "        individual_plans = dict1[county]['individual_data']['merge_variable'].values\n",
    "\n",
    "        shares_vector = np.zeros((m))\n",
    "        \n",
    "        #getting model-predicted choice probability for plan individual actually chooses\n",
    "        for i in range(m):\n",
    "            index = np.where(individual_plans[i] == plans_matrixOO)[0][0]\n",
    "            shares_vector[i] = pcp_full[i,index]\n",
    "        \n",
    "        #taking logs, weighting observatiosn\n",
    "        ln_shares = np.log(shares_vector)\n",
    "        shares_weights = np.multiply(ln_shares, weights)\n",
    "        sum_ln = np.sum(shares_weights)\n",
    "        \n",
    "        #adding to likelihood vector of all counties\n",
    "        likelihood_vec = np.concatenate([likelihood_vec, np.array([sum_ln])])\n",
    "        N = N + m\n",
    "    \n",
    "    #summing the log likelihood over all counties (and scaling by 1/N)\n",
    "    likelihood_sum = (1/N)*np.sum(likelihood_vec)\n",
    "    \n",
    "    return likelihood_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cdd5ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.397685099975206 seconds\n"
     ]
    }
   ],
   "source": [
    "#calculating log-likelihood\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "likelihood_info_2= likelihood_function_b(main_dictionary, usable_county_years, old_guess_x)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "total = end_time-start_time\n",
    "print(f\"elapsed time: {total} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b3c5e",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9100113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(theta1_guess, dict, usable_county_years, tol):\n",
    "    \n",
    "    dict2 = contraction_mapping_b(dict, usable_county_years, theta1_guess, tol)\n",
    "    \n",
    "    if dict2 == \"Bad Parameter Guess\":\n",
    "        return 1e30\n",
    "    \n",
    "    log_likelihood = likelihood_function_b(dict2, usable_county_years, theta1_guess)\n",
    "    log_likelihood_new = -log_likelihood\n",
    "    \n",
    "    return log_likelihood_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fd7474e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 19.488908600003924 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "objective_function_value = objective_function(old_guess_x, main_dictionary, usable_county_years, 1e-10)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "total = end_time-start_time\n",
    "print(f\"elapsed time: {total} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8321c8",
   "metadata": {},
   "source": [
    "### Maximize Objective function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "820720b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimates successfully saved to file\n",
      "Elapsed time: 16.93829968591531 hours\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "##maximizing likelihood function using BFGS method, an initial guess from a toned-down version of the model, and a \n",
    "##contraction mapping tolerance of 1e-10\n",
    "initial_estimates = minimize(objective_function, old_guess_x, args = (main_dictionary, usable_county_years, 1e-10),\n",
    "                            method = \"BFGS\", options={'gtol': 1e-4})\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "##saving estimates to use later \n",
    "with open('initial_estimates_full_2019.pkl', 'wb') as f:\n",
    "    pickle.dump(initial_estimates, f)\n",
    "    print('estimates successfully saved to file')\n",
    "    \n",
    "elapsed_time = (end_time - start_time)/3600\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf76ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###loading the initial parameter estimates\n",
    "\n",
    "with open('initial_estimates_full_2019.pkl', 'rb') as fp:\n",
    "    initial_estimates = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d6231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###dividing the estimates into the parameters that are demographic/plan interactions and those that are just demographic\n",
    "# variables\n",
    "\n",
    "interaction_parameters = (initial_estimates.x[0:24])\n",
    "interaction_parameters_reshaped = interaction_parameters.reshape(4,-1)\n",
    "non_interaction_parameters = initial_estimates.x[24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5c335",
   "metadata": {},
   "source": [
    "### Getting the mean utility guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7ab8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###reloading the main dictionary\n",
    "\n",
    "with open('main_dictionary_2019.pkl', 'rb') as fp:\n",
    "    main_dictionary_a = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f2c2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###updating the dataset with the mean-utility values that correspond to MLE Estimates\n",
    "\n",
    "new_data = contraction_mapping_b(main_dictionary_a, usable_county_years, initial_estimates.x, 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20b6fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GES58\\AppData\\Local\\Temp\\ipykernel_4504\\1026565887.py:7: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    eff_premium_instrument_border_adjusted   ->   eff_premium_instrument_border_ad\n",
      "    premium_instrument_border_adjusted   ->   premium_instrument_border_adjust\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  new_data_frame.to_stata('output_plan_data_2019.dta')\n"
     ]
    }
   ],
   "source": [
    "##recreating data frame of plan level data to export to stata for other uses\n",
    "\n",
    "\n",
    "new_data_frame = pd.DataFrame()\n",
    "\n",
    "for county in usable_county_years:\n",
    "    temp_df = new_data[county]['plan_data']\n",
    "    new_data_frame =pd.concat([new_data_frame, temp_df])\n",
    "    \n",
    "new_data_frame.to_stata('output_plan_data_2019.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c210f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GES58\\AppData\\Local\\Temp\\ipykernel_4504\\3213672992.py:7: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    income_tercile_1.0   ->   income_tercile_1_0\n",
      "    income_tercile_2.0   ->   income_tercile_2_0\n",
      "    income_tercile_3.0   ->   income_tercile_3_0\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  individual_data_frame.to_stata('individual_data_output_2019.dta')\n"
     ]
    }
   ],
   "source": [
    "##recreating data frame of individual level data to export to stata for other uses\n",
    "\n",
    "individual_data_frame = pd.DataFrame()\n",
    "\n",
    "for county in usable_county_years:\n",
    "    temp_df_ind = new_data[county]['individual_data']\n",
    "    individual_data_frame = pd.concat([individual_data_frame, temp_df_ind])\n",
    "\n",
    "individual_data_frame.to_stata('individual_data_output_2019.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b66830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = individual_data_frame.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
